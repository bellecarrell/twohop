{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b3000448-8430-4694-bcef-7150501b5d59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement openai_key (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for openai_key\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install openai_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22338dcd-fe2c-465f-8773-12e38838c606",
   "metadata": {},
   "outputs": [],
   "source": [
    "from benchmark import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "857b3f0b-f8cd-48d4-9ea7-470208d1a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "from wikidata.utils import get_label, load_json, ent_label2id, subject_relation_to_targets, ent_to_relation_ids\n",
    "from wikidata.relations import our_relations\n",
    "from wikidata.recently_modified_facts import recently_modified_facts_given_relation\n",
    "from build_benchmark_tests import \\\n",
    "    making_up_axis, \\\n",
    "    logical_constraints_axis, \\\n",
    "    subject_aliasing_axis, \\\n",
    "    two_hop_axis, \\\n",
    "    forward_two_hop_axis, \\\n",
    "    temporal_axis\n",
    "from relation import Relation\n",
    "from fact import Fact\n",
    "from benchmark import CounterFactualExample, RecentlyAddedExample, Dataset\n",
    "from queryexecutor import QueryExecutor\n",
    "from build_benchmark import sample_relevant_facts_given_list_of_subjects, all_relevant_facts_given_list_of_subjects\n",
    "\n",
    "def two_hop_axis_fact_only(subject_id: str, relation: Relation, target_id: str):\n",
    "    tests = []\n",
    "    if not target_id or target_id[0] != 'Q':\n",
    "        return tests\n",
    "    target_relations = ent_to_relation_ids(target_id)\n",
    "    for relation_id in target_relations:\n",
    "        second_relation_enum = Relation.id_to_enum(relation_id)\n",
    "        if second_relation_enum is None:\n",
    "            continue\n",
    "        second_hop_targets = subject_relation_to_targets(target_id, second_relation_enum)\n",
    "        for second_hop_target in second_hop_targets:\n",
    "            phrase = relation_couple_to_phrase(relation, second_relation_enum)\n",
    "            if phrase is None:\n",
    "                continue\n",
    "            phrase = phrase.replace('<subject>', get_label(subject_id))\n",
    "            test_query = TwoHopQuery(subject_id, relation, target_id, second_relation_enum, second_hop_target, phrase)\n",
    "            tests.append(test_query)\n",
    "\n",
    "    return tests\n",
    "\n",
    "\n",
    "def forward_two_hop_axis_fact_only(subject_id: str, relation: Relation, target_id: str):\n",
    "    tests = []\n",
    "    if not target_id or target_id[0] != 'Q':\n",
    "        return tests\n",
    "    for backward_relation in Relation:\n",
    "        backward_relation_id = backward_relation.id()\n",
    "        backward_subjects = subjects_given_relation_target(backward_relation_id, subject_id)\n",
    "        for backward_subject in backward_subjects:\n",
    "            phrase = relation_couple_to_phrase(backward_relation, relation)\n",
    "            if phrase is None:\n",
    "                continue\n",
    "            phrase = phrase.replace('<subject>', get_label(backward_subject))\n",
    "            test_query = TwoHopQuery(backward_subject, backward_relation, subject_id, relation, target_id, phrase)\n",
    "            tests.append(test_query)\n",
    "    return tests\n",
    "\n",
    "\n",
    "def build_two_hop_fact_dataset_example(subject_id: str, relation: Relation, target_id: str):\n",
    "    fact = Fact(subject_id, relation, target_id)\n",
    "    two_hop_tests = two_hop_axis_fact_only(subject_id, relation, target_id)\n",
    "    forward_two_hop_tests = forward_two_hop_axis_fact_only(subject_id, relation, target_id)\n",
    "    all_two_hop = two_hop_tests.append(forward_two_hop_tests)\n",
    "    return all_two_hop\n",
    "\n",
    "# - [ ] Edit `all_relevant_facts_given_list_of_subjects` to filter based on two hop relation list\n",
    "\n",
    "def all_relevant_facts_given_list_of_subjects(subjects: list, limit: int = None):\n",
    "    facts = []\n",
    "    for i, subject_id in enumerate(subjects):\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'{i+1}/{len(subjects)}')\n",
    "        relevant_relation_ids = ent_to_relation_ids(subject_id)\n",
    "        for relation_id in relevant_relation_ids:\n",
    "            relation_enum = Relation.id_to_enum(relation_id)\n",
    "            if relation_enum is None:\n",
    "                continue\n",
    "            targets = subject_relation_to_targets(subject_id, relation_id)\n",
    "            for target_id in targets:\n",
    "                facts.append((subject_id, relation_enum, target_id))\n",
    "        if limit is not None and len(facts) >= limit:\n",
    "            break\n",
    "    return facts\n",
    "# - [ ] Edit `construct_fake_edits_benchmark` to construct two hop dataset\n",
    "def construct_two_hop_benchmark(size: int = None):\n",
    "    current_data = load_json('./generations/uniformly_from_recent_days_recently_modified_dataset.json')\n",
    "    if size is not None:\n",
    "        current_data = random.sample(current_data, min(size, len(current_data)))\n",
    "    dataset_list = []\n",
    "    i = 0\n",
    "    for subject_id, relation_id, target_id in current_data:\n",
    "        relation_enum = Relation.id_to_enum(relation_id)\n",
    "        if relation_enum is None:\n",
    "            continue\n",
    "        try:\n",
    "            dataset_list.append(build_two_hop_fact_dataset_example(subject_id, relation_enum, target_id))\n",
    "        except:\n",
    "            continue\n",
    "        i += 1\n",
    "        if i % 100 == 0:\n",
    "            print(f'Built {i}/{len(current_data)}')\n",
    "    return Dataset(dataset_list)\n",
    "\n",
    "def construct_two_hop_dataset(path: str, limit: int, facts_limit: int = None,\n",
    "                                                   limit_subjects: int = None, limit_num_of_facts: int = None):\n",
    "    subjects_json = load_json(path)\n",
    "    subject_list = []\n",
    "    for bucket in subjects_json:\n",
    "        subject_list.extend(bucket)\n",
    "    subject_ids = [ent_label2id(subject_label) for subject_label in subject_list]\n",
    "    if limit_subjects is not None:\n",
    "        subject_ids = random.sample(subject_ids, min(limit_subjects, len(subject_ids)))\n",
    "    print('extracting facts..')\n",
    "    if limit_num_of_facts is None:\n",
    "        all_relevant_facts = all_relevant_facts_given_list_of_subjects(subject_ids, facts_limit)\n",
    "    else:\n",
    "        all_relevant_facts = sample_relevant_facts_given_list_of_subjects(subject_ids, limit_num_of_facts, facts_limit)\n",
    "    print(f'have got {len(all_relevant_facts)} relevant facts to sample from')\n",
    "    print('building dataset..')\n",
    "    random.shuffle(all_relevant_facts)\n",
    "    all_relevant_facts = random.sample(all_relevant_facts, min(limit, len(all_relevant_facts)))\n",
    "    dataset = construct_two_hop_benchmark(all_relevant_facts)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa3801a6-f7c2-49f6-811f-74025f5a5309",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extracting facts..\n",
      "have got 10 relevant facts to sample from\n",
      "building dataset..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './generations/uniformly_from_recent_days_recently_modified_dataset.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m two_hop_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m----> 2\u001b[0m two_hop_benchmark \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_two_hop_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./generations/sampled_entities_divided_to_buckets_5000.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtwo_hop_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfacts_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_num_of_facts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit_subjects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m two_hop_benchmark\u001b[38;5;241m.\u001b[39mto_file(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./benchmark/final/two_hop_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtwo_hop_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 120\u001b[0m, in \u001b[0;36mconstruct_two_hop_dataset\u001b[0;34m(path, limit, facts_limit, limit_subjects, limit_num_of_facts)\u001b[0m\n\u001b[1;32m    118\u001b[0m random\u001b[38;5;241m.\u001b[39mshuffle(all_relevant_facts)\n\u001b[1;32m    119\u001b[0m all_relevant_facts \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(all_relevant_facts, \u001b[38;5;28mmin\u001b[39m(limit, \u001b[38;5;28mlen\u001b[39m(all_relevant_facts)))\n\u001b[0;32m--> 120\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mconstruct_two_hop_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_relevant_facts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset\n",
      "Cell \u001b[0;32mIn[4], line 84\u001b[0m, in \u001b[0;36mconstruct_two_hop_benchmark\u001b[0;34m(size)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstruct_two_hop_benchmark\u001b[39m(size: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 84\u001b[0m     current_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_json\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./generations/uniformly_from_recent_days_recently_modified_dataset.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     86\u001b[0m         current_data \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39msample(current_data, \u001b[38;5;28mmin\u001b[39m(size, \u001b[38;5;28mlen\u001b[39m(current_data)))\n",
      "File \u001b[0;32m~/workplace/RippleEdits/src/wikidata/utils.py:13\u001b[0m, in \u001b[0;36mload_json\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_json\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr+\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m     14\u001b[0m         result \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './generations/uniformly_from_recent_days_recently_modified_dataset.json'"
     ]
    }
   ],
   "source": [
    "two_hop_size = 10\n",
    "two_hop_benchmark = construct_two_hop_dataset(\n",
    "    path='./generations/sampled_entities_divided_to_buckets_5000.json',\n",
    "    limit=two_hop_size, facts_limit=10, limit_num_of_facts=10, limit_subjects=10\n",
    ")\n",
    "two_hop_benchmark.to_file(f'./benchmark/final/two_hop_{two_hop_size}.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "35c7adc9-745c-41a0-89e9-f4381292f9fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "532"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from relation import Relation\n",
    "from two_hop_phrases import relation_couple_to_phrase\n",
    "len(Relation)\n",
    "relation_two_hops = []\n",
    "for rel1 in Relation:\n",
    "    for rel2 in Relation:\n",
    "        if relation_couple_to_phrase(rel1, rel2) is not None:\n",
    "            relation_two_hops.append((rel1, rel2))\n",
    "len(relation_two_hops)\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
